{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![alt text](https://avatars1.githubusercontent.com/u/59831504?s=400&v=4 \"MtheEPIC User Icon\")](https://github.com/MtheEPIC/KaggleProj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![alt text](https://avatars1.githubusercontent.com/u/59831504?s=400&v=4 \"MtheEPIC User Icon\")](https://github.com/MtheEPIC/DataImaging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'cross_val_predict' from 'sklearn.metrics' (C:\\Users\\mthee\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-038102e7d4fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinarize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_roc_curve\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_precision_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'cross_val_predict' from 'sklearn.metrics' (C:\\Users\\mthee\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# sns.set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\n",
    "sns.axes_style(\"whitegrid\")\n",
    "\n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.callbacks import TensorBoard\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import tensorflow as tf\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, binarize\n",
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score, plot_roc_curve, confusion_matrix, classification_report, precision_recall_curve, average_precision_score, cross_val_predict\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/fashion-mnist_train.csv')\n",
    "test_df = pd.read_csv('data/fashion-mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 is t shirt\\\n",
    "1 is trousers\\\n",
    "2 is pullover\\\n",
    "3 is dress\\\n",
    "4 is coat\\\n",
    "5 is sandals\\\n",
    "6 is shirt\\\n",
    "7 is sneaker\\\n",
    "8 is bag\\\n",
    "9 is ankle boots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split the images from the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split the training data into train and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(train_df, dtype='float32')\n",
    "test_data = np.array(test_df, dtype='float32')\n",
    "\n",
    "X_train = train_data[:, 1:] / 1\n",
    "y_train = train_data[:, 0]\n",
    "\n",
    "X_test = test_data[:, 1:] / 1#/128#/255\n",
    "y_test = test_data[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "x = np.arange(1, len(cumsum)+1)\n",
    "plt.plot(x, cumsum)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.90)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# print(pca.explained_variance_ratio_)\n",
    "# print(pca.singular_values_)\n",
    "\n",
    "d = len(pca.singular_values_)\n",
    "pca2 = PCA(n_components=d)\n",
    "X_test_pca = pca2.fit_transform(X_test)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run again with pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_image = X_train[0]\n",
    "\n",
    "first_image = np.array(first_image, dtype='uint8')\n",
    "pixels = first_image.reshape((28, 28))\n",
    "plt.imshow(pixels, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paint():\n",
    "    tmp = train_df.groupby('label').sum()\n",
    "    train_data = np.array(tmp, dtype='float32')\n",
    "\n",
    "    first_image = train_data[0]\n",
    "\n",
    "    first_image = np.array(first_image, dtype='uint8')\n",
    "    pixels = first_image.reshape((28, 28))\n",
    "    import numpy as np; np.random.seed(0)\n",
    "\n",
    "    import seaborn as sns; sns.set()\n",
    "\n",
    "    # uniform_data = np.random.rand(10, 12)\n",
    "    uniform_data = pixels#//500\n",
    "    ax = sns.heatmap(uniform_data)\n",
    "\n",
    "    uniform_data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true, pred):\n",
    "#     model_accuracy = roc_auc_score(true, pred, multi_class=\"ovr\")\n",
    "#     null_accuracy = roc_auc_score(true, true.replace(1, 0))\n",
    "    \n",
    "#     print(\"Null AUC Score: {:.5f}\".format(null_accuracy))\n",
    "#     print(\"Model AUC Score: {:.5f}\".format(model_accuracy))\n",
    "\n",
    "#     if null_accuracy >= model_accuracy:\n",
    "#         print(\"The model isn't effective\")\n",
    "# #         return\n",
    "#     else:\n",
    "#         print(\"The model is better then a 'dumb' model\")\n",
    "    print(\"confusion_matrix:\\n\", confusion_matrix(true, pred))\n",
    "    print(classification_report(true, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(predictions)\n",
    "# y_test\n",
    "# evaluate(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(X_train_pca, y_train)\n",
    "predictions = gnb.predict(X_test_pca)\n",
    "evaluate(y_test, predictions)\n",
    "\n",
    "scores = cross_val_score(gnb, X_train_pca, y_train, cv=5)\n",
    "gnb_score = scores\n",
    "\n",
    "model_dict.update({\"GaussianNB\": gnb_score})\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (gnb_score.mean(), gnb_score.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnb = MultinomialNB()\n",
    "# mnb.fit(X_train_pca, y_train)\n",
    "# predictions = mnb.predict(X_test_pca)\n",
    "# evaluate(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=0, max_depth=None, n_estimators=100)\n",
    "\n",
    "# scores = cross_val_score(rf, X_train_pca, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "# rf_score = np.sqrt(-scores)\n",
    "scores = cross_val_score(rf, X_train_pca, y_train, cv=5, n_jobs=-1)\n",
    "rf_score = scores\n",
    "\n",
    "model_dict.update({\"RandomForestClassifier\": rf_score})\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (rf_score.mean(), rf_score.std() * 2))\n",
    "\n",
    "tmp = cross_val_predict(rf, X_train_pca, y_train, cv=5, n_jobs=-1)\n",
    "\n",
    "\n",
    "print(len(predictions))\n",
    "print(len(tmp))\n",
    "# y_test_appended = np.append(np.append(y_test,y_test),np.append(np.append(y_test, y_test),np.append(y_test, y_test)))\n",
    "evaluate(y_train, tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = rf.predict(X_train_pca)\n",
    "evaluate(y_train, predictions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "tmp2 = cross_val_predict(rf, X_train_pca, y_train, cv=5, n_jobs=-1)\n",
    "tmp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets find the optimum K \n",
    "k_max = 20\n",
    "k_min = 1\n",
    "err_rate = []\n",
    "for i in range(k_min, k_max+1):\n",
    "    k = KNeighborsClassifier(n_neighbors=i)\n",
    "#     k.fit(X_train_pca, y_train)\n",
    "#     pred_i = k.predict(X_test_pca)\n",
    "#     err_rate.append(np.mean(pred_i != y_test))\n",
    "    scores = cross_val_score(k, X_train_pca, y_train, cv=5, n_jobs=-1)\n",
    "    if scores.std() > 0.1:\n",
    "        err_rate.append(0)\n",
    "    else:\n",
    "        err_rate.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(k_min, k_max+1), err_rate)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = err_rate.index(min(err_rate)) + 1\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = KNeighborsClassifier(n_neighbors=index)\n",
    "# k.fit(X_train_pca, y_train)\n",
    "# predictions = k.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_scores = cross_val_score(k, X_train_pca, y_train, cv=5)\n",
    "model_dict.update({\"KNN\": k_scores})\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (k_scores.mean(), k_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=0)#, solver='lbfgs', penalty='l2')\n",
    "# lr2.fit(X_train_pca, y_train)\n",
    "# predictions = lr2.predict(X_train_pca)\n",
    "# evaluate(y_train, predictions)\n",
    "\n",
    "lr_scores = cross_validate(lr2, X_train_pca, y_train, cv=5, n_jobs=-1)\n",
    "\n",
    "model_dict.update({\"logistic regression\": lr_scores})\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (lr_scores['test_score'].mean(), lr_scores['test_score'].std() * 2))\n",
    "print('the mean train time is {} seconds'.format(lr_scores['fit_time'].mean()))\n",
    "print('the mean test time is {} seconds'.format(lr_scores['score_time'].mean()))\n",
    "# lr2_score = scores\n",
    "# print(\"Accuracy: %0.2f (+/- %0.2f)\" % (lr2_score.mean(), lr2_score.std() * 2))\n",
    "\n",
    "# tmp = cross_val_predict(lr2, X_train_pca, y_train, cv=5, n_jobs=-1)\n",
    "# tmp\n",
    "\n",
    "# print(len(predictions))\n",
    "# print(len(tmp))\n",
    "# # y_test_appended = np.append(np.append(y_test,y_test),np.append(np.append(y_test, y_test),np.append(y_test, y_test)))\n",
    "# evaluate(y_train, tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(random_state=0, max_iter=500)\n",
    "# svc.fit(X_train, y_train)\n",
    "# predictions = svc.predict(X_test)\n",
    "# evaluate(y_test, predictions)\n",
    "\n",
    "svc_scores = cross_validate(svc, X_train_pca, y_train, cv=5, n_jobs=-1)\n",
    "model_dict.update({\"SVC\": svc_scores})\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (svc_scores['test_score'].mean(), svc_scores['test_score'].std() * 2))\n",
    "print('the mean train time is {} seconds'.format(svc_scores['fit_time'].mean()))\n",
    "print('the mean test time is {} seconds'.format(svc_scores['score_time'].mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "# dtc.fit(X_train, y_train)\n",
    "# predictions = dtc.predict(X_test)\n",
    "# evaluate(y_test, predictions)\n",
    "\n",
    "dtc_scores = cross_validate(svc, X_train_pca, y_train, cv=5, n_jobs=-1)\n",
    "model_dict.update({\"decision tree\": dtc_scores})\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (dtc_scores['test_score'].mean(), dtc_scores['test_score'].std() * 2))\n",
    "print('the mean train time is {} seconds'.format(dtc_scores['fit_time'].mean()))\n",
    "print('the mean test time is {} seconds'.format(dtc_scores['score_time'].mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_model():\n",
    "    max_score = 0\n",
    "    best_key = None\n",
    "    for key in model_dict:\n",
    "        model_score = model_dict[key].mean()\n",
    "        if max_score < model_score:\n",
    "            max_score = model_score\n",
    "            best_key = key\n",
    "    print('the best model is {} with the score of {}'.format(best_key, max_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
